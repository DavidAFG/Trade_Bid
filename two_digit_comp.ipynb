{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b0afcbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.13/site-packages (2.3.3)\n",
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.13/site-packages (from pandas) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [openpyxl]\n",
      "\u001b[1A\u001b[2KSuccessfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3e23a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserting libraries to work\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d535bc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe\n",
    "db_23 = pd.read_csv('/Volumes/Toshi 1Tb/Trade/Trade Correo/tradeInGoods_baci_2023.csv', sep=\";\", encoding='latin1')\n",
    "db_23[\"year\"] = \"2023\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ba3fc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecesary columns for speed. Dictionary is in excel\n",
    "db_23_clean = db_23.drop(columns=['country_desciption', 'partner_desciption', 'item_description'])\n",
    "\n",
    "# Changing european , for .\n",
    "db_23_clean[\"value\"] = (db_23_clean[\"value\"].astype(str).str.replace(\",\", \".\", regex=False))\n",
    "db_23_clean[\"value\"] = pd.to_numeric(db_23_clean[\"value\"], errors=\"coerce\")\n",
    "\n",
    "# Creating new variable that is the first two digits of the item code\n",
    "db_23_clean[\"item2\"] = db_23_clean[\"item_code\"].astype(str).str[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c8e3096",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting database into exporter/importer\n",
    "db_23_imp = db_23_clean[db_23_clean[\"flow\"] == \"I\"].copy()\n",
    "db_23_exp = db_23_clean[db_23_clean[\"flow\"] == \"E\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6314cbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group at the 2 digit level\n",
    "grouped_exp = (db_23_exp.groupby([\"year\", \"country_code\", \"partner_code\", \"item2\"], as_index=False)[\"value\"].sum())\n",
    "grouped_imp = (db_23_imp.groupby([\"year\", \"country_code\", \"partner_code\", \"item2\"], as_index=False)[\"value\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605e60b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>country_code</th>\n",
       "      <th>partner_code</th>\n",
       "      <th>item2</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>ABW</td>\n",
       "      <td>AGO</td>\n",
       "      <td>61</td>\n",
       "      <td>5081.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023</td>\n",
       "      <td>ABW</td>\n",
       "      <td>AGO</td>\n",
       "      <td>84</td>\n",
       "      <td>1519.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023</td>\n",
       "      <td>ABW</td>\n",
       "      <td>ARE</td>\n",
       "      <td>10</td>\n",
       "      <td>408.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023</td>\n",
       "      <td>ABW</td>\n",
       "      <td>ARE</td>\n",
       "      <td>22</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023</td>\n",
       "      <td>ABW</td>\n",
       "      <td>ARE</td>\n",
       "      <td>30</td>\n",
       "      <td>6969.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year country_code partner_code item2   value\n",
       "0  2023          ABW          AGO    61  5081.0\n",
       "1  2023          ABW          AGO    84  1519.0\n",
       "2  2023          ABW          ARE    10   408.0\n",
       "3  2023          ABW          ARE    22   160.0\n",
       "4  2023          ABW          ARE    30  6969.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base Code before loop to test and replicate in for-loop\n",
    "\n",
    "# Load the dataframe\n",
    "db_23 = pd.read_csv('/Volumes/Toshi 1Tb/Trade/Trade Correo/tradeInGoods_baci_2023.csv', sep=\";\", encoding='latin1')\n",
    "db_23[\"year\"] = \"2023\"\n",
    "\n",
    "# Dropping unnecesary columns for speed. Dictionary is in excel\n",
    "db_23_clean = db_23.drop(columns=['country_desciption', 'partner_desciption', 'item_description'])\n",
    "\n",
    "# Changing european , for .\n",
    "db_23_clean[\"value\"] = (db_23_clean[\"value\"].astype(str).str.replace(\",\", \".\", regex=False))\n",
    "db_23_clean[\"value\"] = pd.to_numeric(db_23_clean[\"value\"], errors=\"coerce\")\n",
    "\n",
    "# Creating new variable that is the first two digits of the item code\n",
    "db_23_clean[\"item2\"] = db_23_clean[\"item_code\"].astype(str).str[:2]\n",
    "\n",
    "#Splitting database into exporter/importer\n",
    "db_23_imp = db_23_clean[db_23_clean[\"flow\"] == \"I\"].copy()\n",
    "db_23_exp = db_23_clean[db_23_clean[\"flow\"] == \"E\"].copy()\n",
    "\n",
    "# Group at the 2 digit level\n",
    "grouped_exp = (db_23_exp.groupby([\"year\", \"country_code\", \"partner_code\", \"item2\"], as_index=False)[\"value\"].sum())\n",
    "grouped_imp = (db_23_imp.groupby([\"year\", \"country_code\", \"partner_code\", \"item2\"], as_index=False)[\"value\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fad046c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1996...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# ========== 1. Load CSV ==========\u001b[39;00m\n\u001b[32m     16\u001b[39m file_path = data_path / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtradeInGoods_baci_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m db = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m;\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlatin1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_bad_lines\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mskip\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Added on_bad_lines skip because of a line on dataset 2009, that crashed the loop becuase it had 12 instead of 11 columns\u001b[39;00m\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# ========== 2. Drop unnecessary columns ==========\u001b[39;00m\n\u001b[32m     21\u001b[39m db_clean = db.drop(columns=[\n\u001b[32m     22\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcountry_desciption\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     23\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mpartner_desciption\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     24\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mitem_description\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     25\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     26\u001b[39m ])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/BID/Trade/Trade_Bid/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/BID/Trade/Trade_Bid/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/BID/Trade/Trade_Bid/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/BID/Trade/Trade_Bid/.venv/lib/python3.13/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2053\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/encodings/latin_1.py:25\u001b[39m, in \u001b[36mIncrementalDecoder.decode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIncrementalDecoder\u001b[39;00m(codecs.IncrementalDecoder):\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m     26\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m codecs.latin_1_decode(\u001b[38;5;28minput\u001b[39m,\u001b[38;5;28mself\u001b[39m.errors)[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Folder containing all the CSV files\n",
    "data_path = Path('/Volumes/Toshi 1Tb/Trade/Trade Correo/')\n",
    "\n",
    "# Years you want to process\n",
    "years = range(1996, 2023)\n",
    "\n",
    "key_cols = [\"country_code\", \"partner_code\", \"item2\"]\n",
    "\n",
    "all_exports = None\n",
    "all_imports = None\n",
    "\n",
    "for year in years:\n",
    "    print(f\"Processing {year}...\")\n",
    "\n",
    "    # ========== 1. Load CSV ==========\n",
    "    file_path = data_path / f\"tradeInGoods_baci_{year}.csv\"\n",
    "    db = pd.read_csv(file_path, sep=\";\", encoding='latin1', dtype = str, on_bad_lines=\"skip\")\n",
    "    # Added on_bad_lines skip because of a line on dataset 2009, that crashed the loop becuase it had 12 instead of 11 columns\n",
    "\n",
    "    # ========== 2. Drop unnecessary columns ==========\n",
    "    db_clean = db.drop(columns=[\n",
    "        'country_desciption',\n",
    "        'partner_desciption',\n",
    "        'item_description',\n",
    "        'date'\n",
    "    ])\n",
    "\n",
    "    # ========== 3. Fix decimal commas ==========\n",
    "    db_clean[\"value\"] = (db_clean[\"value\"].astype(str).str.replace(\",\", \".\", regex=False))\n",
    "    db_clean[\"item_code\"] = (db_clean[\"item_code\"].astype(str).str.replace(\",\", \".\", regex=False))\n",
    "    db_clean[\"net_weight\"] = (db_clean[\"net_weight\"].astype(str).str.replace(\",\", \".\", regex=False))\n",
    "\n",
    "    db_clean[\"value\"] = pd.to_numeric(db_clean[\"value\"], errors=\"coerce\")\n",
    "    db_clean[\"item_code\"] = pd.to_numeric(db_clean[\"item_code\"], errors=\"coerce\")\n",
    "    db_clean[\"net_weight\"] = pd.to_numeric(db_clean[\"net_weight\"], errors=\"coerce\")\n",
    "\n",
    "    # ========== 4. Extract first two digits of item code ==========\n",
    "    db_clean[\"item2\"] = db_clean[\"item_code\"].astype(str).str[:2]\n",
    "\n",
    "    # ========== 5. Split into importer / exporter ==========\n",
    "    db_imp = db_clean[db_clean[\"flow\"] == \"I\"].copy()\n",
    "    db_exp = db_clean[db_clean[\"flow\"] == \"E\"].copy()\n",
    "\n",
    "    # ========== 6. Group at 2-digit item level ==========\n",
    "    grouped_exp = (db_exp.groupby(key_cols, as_index=False)[\"value\"].sum())\n",
    "\n",
    "    grouped_imp = (db_imp.groupby(key_cols, as_index=False)[\"value\"].sum())\n",
    "\n",
    "    # Store results in lists\n",
    "    year_col = str(year)\n",
    "    grouped_exp = grouped_exp.rename(columns={\"value\": year_col})\n",
    "    grouped_imp = grouped_imp.rename(columns={\"value\": year_col})\n",
    "\n",
    "    # 8. Full outer join with accumulated panel (one column per year)\n",
    "\n",
    "    # Exports\n",
    "    if exports_panel is None:\n",
    "        exports_panel = grouped_exp\n",
    "    else:\n",
    "        exports_panel = exports_panel.merge(grouped_exp, on=key_cols, how=\"outer\")\n",
    "\n",
    "    # Imports\n",
    "    if imports_panel is None:\n",
    "        imports_panel = grouped_imp\n",
    "    else:\n",
    "        imports_panel = imports_panel.merge(grouped_imp, on=key_cols, how=\"outer\")\n",
    "\n",
    "# 9. (Optional) save the final wide panels\n",
    "exports_panel.to_csv(data_path / \"exports_CEPII_twodig.csv\", index=False)\n",
    "imports_panel.to_csv(data_path / \"imports_CEPII_twodig.csv\", index=False)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7434728a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
